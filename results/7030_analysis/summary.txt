======================================================================
70/30 FEDERATED LEARNING ANALYSIS – KEY RESULTS
======================================================================

Setup
  2 clients: Client 0 has 70% data (166 train), Client 1 has 30% (73 train)
  BraTS 2D U-Net, BCE+Dice loss, Adam lr=1e-3, seed=42
  Federated: 30 rounds x 3 local epochs = 90 effective epochs
  Local:     30 epochs (same total training iterations for Client 0)

──────────────────────────────────────────────────────────────────────
LOCAL-ONLY BASELINES (best-val-checkpoint, evaluated on test set)
──────────────────────────────────────────────────────────────────────
  Client 0: Mean=0.5069  WT=0.5577  TC=0.4344  ET=0.5284
  Client 1: Mean=0.4164  WT=0.4837  TC=0.4478  ET=0.3178

──────────────────────────────────────────────────────────────────────
FEDERATED RESULTS (best round, evaluated on each client's test set)
──────────────────────────────────────────────────────────────────────
  FedAvg:
    Client 0: best=0.7440  final=0.7320
    Client 1: best=0.8126  final=0.6700
    Global:   best=0.7636  final=0.7143

  FedProx (mu=0.001):
    Client 0: best=0.7726  final=0.6860
    Client 1: best=0.7724  final=0.6807
    Global:   best=0.7582  final=0.6845

──────────────────────────────────────────────────────────────────────
KEY FINDINGS
──────────────────────────────────────────────────────────────────────

1. FEDERATION DRAMATICALLY HELPS BOTH CLIENTS

   Client 0 (70% data, 166 samples):
     Local test Dice:  0.5069
     FedAvg best:      0.7440  (+0.2371, +46.8%)
     FedProx best:     0.7726  (+0.2657)

   Client 1 (30% data, 73 samples):
     Local test Dice:  0.4164
     FedAvg best:      0.8126  (+0.3962, +95.2%)
     FedProx best:     0.7724  (+0.3560)

   Federation provides a LARGER boost to the data-scarce Client 1:
     C1 gains 0.3962 from FedAvg vs C0's 0.2371.
     The small client benefits most from knowledge shared by the larger client.

2. LOCAL MODELS SEVERELY OVERFIT

   Client 0 local: train Dice reaches 0.6723 but test = 0.5069
     -> generalization gap = 0.1654
   Client 1 local: train Dice reaches 0.6036 but test = 0.4164
     -> generalization gap = 0.1872

   Federation acts as an implicit regularizer: aggregating weights from
   multiple clients prevents any single client from overfitting to its
   local data distribution.

3. PER-CLASS IMPROVEMENTS (FedAvg best round vs Local test)

   Client 0:  WT +0.1754  TC +0.3016  ET +0.2344
   Client 1:  WT +0.3258  TC +0.4145  ET +0.4483

   All three tumor classes improve for both clients.
   Client 1 sees the largest gains across all classes.

4. FEDAVG vs FEDPROX (low heterogeneity regime)

   Global best:  FedAvg=0.7636  FedProx=0.7582  (diff=0.0054)
   With only 2 clients and mild quantity skew (70/30, same BraTS
   distribution), there is minimal client drift for FedProx to correct.
   Both strategies perform comparably.

======================================================================
