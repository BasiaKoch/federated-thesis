==============================================
BraTS 2D U-Net (centralized) baseline
Job ID: 21235405
Node(s): gpu-q-8
Script: /home/bk489/federated/federated-thesis/unet/unet_new.py
Data:   /home/bk489/federated/federated-thesis/data/brats2020_top10_slices_split_npz
Python: /home/bk489/.conda/envs/fed/bin/python
Python 3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) 
[GCC 11.3.0]
PyTorch 2.8.0+cu128 CUDA available: True CUDA: 12.8
Sun Jan 25 19:20:28 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:41:00.0 Off |                    0 |
| N/A   33C    P0             64W /  500W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
==============================================
Num npz files:
3680
Example files:
/home/bk489/federated/federated-thesis/data/brats2020_top10_slices_split_npz/test/BraTS20_Training_053/slice_091_rank03.npz
/home/bk489/federated/federated-thesis/data/brats2020_top10_slices_split_npz/test/BraTS20_Training_053/slice_094_rank08.npz
/home/bk489/federated/federated-thesis/data/brats2020_top10_slices_split_npz/test/BraTS20_Training_053/slice_087_rank04.npz
/home/bk489/federated/federated-thesis/data/brats2020_top10_slices_split_npz/test/BraTS20_Training_053/slice_093_rank06.npz
/home/bk489/federated/federated-thesis/data/brats2020_top10_slices_split_npz/test/BraTS20_Training_053/slice_089_rank01.npz
Starting BraTS U-Net training...
Device: cuda
Train slices: 2940 | Val slices: 370 | Test slices: 370
Saving best model to: runs_unet_brats2d/best_unet2d.pt
Epoch 001/30 | train loss 0.6952 meanDice 0.6870 (WT 0.8360 TC 0.6190 ET 0.6061) | val loss 0.4785 meanDice 0.7750 (WT 0.8622 TC 0.7132 ET 0.7496) | 24.5s
  -> saved new best (val meanDice=0.7750)
Epoch 002/30 | train loss 0.3670 meanDice 0.7708 (WT 0.8793 TC 0.7283 ET 0.7048) | val loss 0.3859 meanDice 0.8220 (WT 0.8961 TC 0.7890 ET 0.7808) | 22.1s
  -> saved new best (val meanDice=0.8220)
Epoch 003/30 | train loss 0.3307 meanDice 0.7868 (WT 0.8899 TC 0.7546 ET 0.7158) | val loss 0.4089 meanDice 0.8071 (WT 0.8695 TC 0.7714 ET 0.7802) | 22.2s
Epoch 004/30 | train loss 0.3060 meanDice 0.8005 (WT 0.8956 TC 0.7754 ET 0.7304) | val loss 0.4348 meanDice 0.7431 (WT 0.8811 TC 0.6904 ET 0.6578) | 22.2s
Epoch 005/30 | train loss 0.2937 meanDice 0.8056 (WT 0.9005 TC 0.7838 ET 0.7325) | val loss 0.3472 meanDice 0.8250 (WT 0.9066 TC 0.8098 ET 0.7585) | 22.1s
  -> saved new best (val meanDice=0.8250)
Epoch 006/30 | train loss 0.2819 meanDice 0.8156 (WT 0.9059 TC 0.7998 ET 0.7412) | val loss 0.4229 meanDice 0.7706 (WT 0.8692 TC 0.7271 ET 0.7155) | 22.3s
Epoch 007/30 | train loss 0.2704 meanDice 0.8211 (WT 0.9102 TC 0.8058 ET 0.7473) | val loss 0.3675 meanDice 0.8064 (WT 0.8946 TC 0.7689 ET 0.7558) | 22.2s
Epoch 008/30 | train loss 0.2646 meanDice 0.8256 (WT 0.9106 TC 0.8101 ET 0.7560) | val loss 0.3693 meanDice 0.8248 (WT 0.8980 TC 0.7845 ET 0.7918) | 22.1s
Epoch 009/30 | train loss 0.2601 meanDice 0.8288 (WT 0.9122 TC 0.8152 ET 0.7590) | val loss 0.3521 meanDice 0.8189 (WT 0.9083 TC 0.7874 ET 0.7611) | 22.1s
Epoch 010/30 | train loss 0.2544 meanDice 0.8307 (WT 0.9146 TC 0.8180 ET 0.7595) | val loss 0.3620 meanDice 0.7995 (WT 0.9033 TC 0.7687 ET 0.7266) | 22.4s
Epoch 011/30 | train loss 0.2488 meanDice 0.8337 (WT 0.9149 TC 0.8254 ET 0.7609) | val loss 0.3653 meanDice 0.7933 (WT 0.9043 TC 0.7463 ET 0.7292) | 22.2s
Epoch 012/30 | train loss 0.2463 meanDice 0.8383 (WT 0.9172 TC 0.8296 ET 0.7682) | val loss 0.3253 meanDice 0.8449 (WT 0.9136 TC 0.8262 ET 0.7949) | 22.2s
  -> saved new best (val meanDice=0.8449)
Epoch 013/30 | train loss 0.2391 meanDice 0.8429 (WT 0.9197 TC 0.8367 ET 0.7722) | val loss 0.3139 meanDice 0.8513 (WT 0.9173 TC 0.8350 ET 0.8016) | 22.3s
  -> saved new best (val meanDice=0.8513)
Epoch 014/30 | train loss 0.2362 meanDice 0.8441 (WT 0.9205 TC 0.8378 ET 0.7739) | val loss 0.3277 meanDice 0.8342 (WT 0.9133 TC 0.8095 ET 0.7800) | 22.2s
Epoch 015/30 | train loss 0.2296 meanDice 0.8470 (WT 0.9216 TC 0.8450 ET 0.7743) | val loss 0.3211 meanDice 0.8468 (WT 0.9144 TC 0.8281 ET 0.7979) | 22.5s
Epoch 016/30 | train loss 0.2260 meanDice 0.8505 (WT 0.9234 TC 0.8519 ET 0.7762) | val loss 0.3467 meanDice 0.8301 (WT 0.9049 TC 0.8048 ET 0.7807) | 22.1s
Epoch 017/30 | train loss 0.2275 meanDice 0.8478 (WT 0.9222 TC 0.8476 ET 0.7735) | val loss 0.3399 meanDice 0.8178 (WT 0.9133 TC 0.7907 ET 0.7494) | 22.6s
Epoch 018/30 | train loss 0.2174 meanDice 0.8530 (WT 0.9252 TC 0.8533 ET 0.7806) | val loss 0.3484 meanDice 0.8211 (WT 0.9117 TC 0.7969 ET 0.7548) | 22.2s
Epoch 019/30 | train loss 0.2202 meanDice 0.8534 (WT 0.9245 TC 0.8538 ET 0.7819) | val loss 0.3335 meanDice 0.8215 (WT 0.9132 TC 0.7904 ET 0.7609) | 22.5s
Epoch 020/30 | train loss 0.2193 meanDice 0.8532 (WT 0.9245 TC 0.8544 ET 0.7806) | val loss 0.3470 meanDice 0.8160 (WT 0.9201 TC 0.7794 ET 0.7484) | 22.4s
Epoch 021/30 | train loss 0.2155 meanDice 0.8563 (WT 0.9257 TC 0.8589 ET 0.7844) | val loss 0.3143 meanDice 0.8390 (WT 0.9192 TC 0.8111 ET 0.7867) | 22.2s
Epoch 022/30 | train loss 0.2136 meanDice 0.8552 (WT 0.9265 TC 0.8607 ET 0.7782) | val loss 0.3084 meanDice 0.8417 (WT 0.9250 TC 0.8334 ET 0.7666) | 22.2s
Epoch 023/30 | train loss 0.2078 meanDice 0.8594 (WT 0.9278 TC 0.8632 ET 0.7871) | val loss 0.3051 meanDice 0.8567 (WT 0.9266 TC 0.8413 ET 0.8024) | 22.2s
  -> saved new best (val meanDice=0.8567)
Epoch 024/30 | train loss 0.2059 meanDice 0.8606 (WT 0.9298 TC 0.8658 ET 0.7864) | val loss 0.3139 meanDice 0.8517 (WT 0.9230 TC 0.8324 ET 0.7998) | 22.2s
Epoch 025/30 | train loss 0.2052 meanDice 0.8598 (WT 0.9276 TC 0.8627 ET 0.7892) | val loss 0.3249 meanDice 0.8464 (WT 0.9173 TC 0.8282 ET 0.7936) | 22.1s
Epoch 026/30 | train loss 0.2044 meanDice 0.8633 (WT 0.9299 TC 0.8700 ET 0.7900) | val loss 0.3113 meanDice 0.8513 (WT 0.9258 TC 0.8332 ET 0.7949) | 23.3s
Epoch 027/30 | train loss 0.2023 meanDice 0.8644 (WT 0.9309 TC 0.8682 ET 0.7940) | val loss 0.3020 meanDice 0.8543 (WT 0.9242 TC 0.8307 ET 0.8080) | 23.5s
Epoch 028/30 | train loss 0.1983 meanDice 0.8655 (WT 0.9307 TC 0.8736 ET 0.7923) | val loss 0.3330 meanDice 0.8381 (WT 0.9172 TC 0.8210 ET 0.7761) | 23.2s
Epoch 029/30 | train loss 0.1988 meanDice 0.8676 (WT 0.9310 TC 0.8719 ET 0.7999) | val loss 0.3037 meanDice 0.8536 (WT 0.9242 TC 0.8307 ET 0.8059) | 23.5s
Epoch 030/30 | train loss 0.1982 meanDice 0.8677 (WT 0.9317 TC 0.8745 ET 0.7969) | val loss 0.2961 meanDice 0.8570 (WT 0.9291 TC 0.8489 ET 0.7931) | 23.0s
  -> saved new best (val meanDice=0.8570)

=== Best checkpoint ===
Epoch: 30 | val meanDice: 0.8570398060619375
=== Test results ===
test loss 0.3288 | meanDice 0.8515 (WT 0.9296 TC 0.8242 ET 0.8007)
Job completed!
